basline (trained, "baseline"): submitted
bidaf   (trained, "bidaf")
bidaf + smart get_start(K=15) : submitted // {"f1": 52.350024821759746, "exact_match": 44.19753086419753} (improvements1+2)
stacked bidaf (n=3) (trained, "stacked") // {"f1": 50.036426171559775, "exact_match": 43.08641975308642} (FIS)
added DrQA features (num_feats=4) note this was still shared encoder, smart get_start(K=15), one-layer RNN (trained, "feat") // {"f1": 65.48; "exact_match": 56.54} (HWG HWG)
added aligned question embedding, SEPARATE ENCODERS, smart get_start(K=15), one-layer RNN  (trained, "aligned")
aligned question embedding, SHARED ENCODERS, smart get_start(K=15), one-layer RNN (trained, "aligned_share") -- loss curve looks identical to separate encoders
self attention, batch size:10, hidden layer:100, context_len:300 (trained, "self") -- higher loss, memory issues
no more self attention, added NER features (trained, "NER") -- about the same as without NER, but 5x slower
added tf feature (no more NER) (trained, "tf") -- looks sliiiiightly worse than aligned... let me try separate encoders
separate encoders (trained, "tf_sep") -- at 18K step, change dropout 0.15->0.30 and max_grad_norm 5.0->3.0 - 
	***NOTE*** dummy features were added to question // {"f1": 60.11107793369007, "exact_match": 51.851851851851855}
~~ start saving best model based on f1/em average ~~
add another RNNencoder (modeling layer) after bidaf layer, use separated encoders (remove unneccesary dummy features from Q), RNNs are 2-stack

other things:
- keep word embeddings fixed, except for 1000 most common question words

"feat model"
K=15: {"f1": 65.48; "exact_match": 56.54}
K=12: {"f1": 66.00372508746105, "exact_match": 57.160493827160494}
K=10: {"f1": 66.49116700082888, "exact_match": 57.65432098765432}
K=8:  {"f1": 66.57032473699145, "exact_match": 58.148148148148145}
K=7:  {"f1": 66.67434965583116, "exact_match": 58.51851851851852}
K=6:  {"f1": 66.51966569997764, "exact_match": 58.51851851851852}
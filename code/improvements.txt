basline (trained, "baseline"): submitted
bidaf   (trained, "bidaf")
bidaf + smart get_start(K=15) : submitted // {"f1": 52.350024821759746, "exact_match": 44.19753086419753} (improvements1+2)
stacked bidaf (n=3) (trained, "stacked") // {"f1": 50.036426171559775, "exact_match": 43.08641975308642} (FIS)
added DrQA features (num_feats=4) note this was still shared encoder, smart get_start(K=15), one-layer RNN (trained, "feat") // {"f1": 65.48; "exact_match": 56.54} (HWG HWG)
added aligned question embedding, SEPARATE ENCODERS, smart get_start(K=15), one-layer RNN  (trained, "aligned")
aligned question embedding, SHARED ENCODERS, smart get_start(K=15), one-layer RNN (trained, "aligned_share") -- loss curve looks identical to separate encoders
self attention, batch size:10, hidden layer:100, context_len:300 (trained, "self") -- higher loss, memory issues
no more self attention, added NER features (trained, "NER") -- about the same as without NER, but 5x slower
added tf feature (no more NER) (trained, "tf") -- looks sliiiiightly worse than aligned... let me try separate encoders
separate encoders (trained, "tf_sep") -- at 18K step, change dropout 0.15->0.30 and max_grad_norm 5.0->3.0 - 
	***NOTE*** dummy features were added to question // {"f1": 60.11107793369007, "exact_match": 51.851851851851855}
~~ start saving best model based on f1/em average ~~
use 300 conext_length, double hidden by 200 -- model overfits. (trained, "feat2h")
FIS (stacked LSTMs, x2 // 200 length embeddings, HS 400 // no modeling layer): 
	K=15: {"f1": 66.96108380700296, "exact_match": 58.51851851851852}
	k=10: {"f1": 67.05299052853933, "exact_match": 58.641975308641975}
cnn (same architecture as BiDAF) (trained, "cnn")
*** NOTE: ALIGNED QUESTION EMBEDDINGS SHOULD HAVE BEEN USING A SINGLE DENSE LAYER WITH RELU ***
allow 1000 most commonQ words to be re-learned
*** NOTE: WAS NOT USING CHARCNN FOR QUERY ***
now using charQ cnn + dense layer in aligned question embedding
cnnQ
cnnQ_rnn2
*** NOTE: not using the learned commonQ words for context... ***
now also using common embeddings for context, fixed EM be ==1, shared encoder
sharedE -- 20K through realized my start dense was non-linear (relu)
	@23.5, learningrate = 0.05, then 0.01 for 2.5 days
	K=20: {"f1": 71.03286320353247, "exact_match": 59.92431409649953}
	K=15: {"f1": 70.99408517205993, "exact_match": 60.00946073793756} (full dev)
	K=10: {"f1": 70.90604994051326, "exact_match": 60.0}


"feat model"
K=15: {"f1": 65.48; "exact_match": 56.54}
K=12: {"f1": 66.00372508746105, "exact_match": 57.160493827160494}
K=10: {"f1": 66.49116700082888, "exact_match": 57.65432098765432}
K=8:  {"f1": 66.57032473699145, "exact_match": 58.148148148148145}
K=7:  {"f1": 66.67434965583116, "exact_match": 58.51851851851852}
K=6:  {"f1": 66.51966569997764, "exact_match": 58.51851851851852}

{"f1": 64.71202809896772, "exact_match": 55.06172839506173}

>>> nltk.download('averaged_perceptron_tagger')
>>> nltk.download('wordnet')
>>> nltk.download('words')
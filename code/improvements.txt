basline (trained, "baseline"): submitted
bidaf   (trained, "bidaf")
bidaf + smart get_start(K=15) : submitted // {"f1": 52.350024821759746, "exact_match": 44.19753086419753} (improvements1+2)
stacked bidaf (n=3) (trained, "stacked") // {"f1": 50.036426171559775, "exact_match": 43.08641975308642} (FIS)
added DrQA features (num_feats=4) note this was still shared encoder, smart get_start(K=15), one-layer RNN (trained, "feat") // {"f1": 65.48; "exact_match": 56.54} (HWG HWG)
added aligned question embedding, SEPARATE ENCODERS, smart get_start(K=15), one-layer RNN  (trained, "aligned")
aligned question embedding, SHARED ENCODERS, smart get_start(K=15), one-layer RNN (trained, "aligned_share") -- loss curve looks identical to separate encoders
self attention, batch size:10, hidden layer:100, context_len:300 (trained, "self") -- higher loss, memory issues
no more self attention, added NER features (trained, "NER") -- about the same as without NER, but 5x slower
added tf feature (no more NER) (trained, "tf") -- looks sliiiiightly worse than aligned... let me try separate encoders
separate encoders (trained, "tf_sep") -- at 18K step, change dropout 0.15->0.30 and max_grad_norm 5.0->3.0 - 
	***NOTE*** dummy features were added to question // {"f1": 60.11107793369007, "exact_match": 51.851851851851855}

add another RNNencoder (modeling layer) after bidaf layer, remove unneccesary dummy features

other things:
- keep word embeddings fixed, except for 1000 most common question words
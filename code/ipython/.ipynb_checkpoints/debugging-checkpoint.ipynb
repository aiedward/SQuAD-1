{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "import numpy as np\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import time\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = tf.constant(np.arange(0, 30, dtype=np.int32), shape=[2, 5, 3])\n",
    "# b = tf.constant(np.arange(0, 24, dtype=np.int32), shape=[2, 3, 4])\n",
    "# c = tf.matmul(a, b)\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sess = tf.Session()\n",
    "# print(sess.run(a[0]))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(b[0]))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(c))\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x = tf.constant(np.arange(0, 30, dtype=np.float),shape=[2, 5, 3])\n",
    "# x2 = tf.nn.softmax(x,axis=2)\n",
    "\n",
    "# sess = tf.Session()\n",
    "# print(sess.run(x[0]))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(x2[0]))\n",
    "# print('~~~~~~~~')\n",
    "# print(np.sum(sess.run(x2[0]),axis=1))\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y = tf.constant(np.arange(0, 10, dtype=np.float), shape=[2, 5, 1])\n",
    "# z = tf.constant(np.arange(0, 15, dtype=np.float), shape=[1, 5, 3])\n",
    "# w = tf.multiply(y,z)\n",
    "\n",
    "# sess = tf.Session()\n",
    "# print(sess.run(y))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(z))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(w))\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = np.random.permutation(24).reshape(2,3,4)\n",
    "# print(a[0])\n",
    "# print(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# idx = np.transpose(np.asarray([np.unravel_index(np.argmax(x, axis=None), x.shape) for x in a]))\n",
    "# print(idx)\n",
    "# row_max = idx[0]\n",
    "# col_max = idx[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# h = 100\n",
    "# rnn_cell_fw = [rnn_cell.GRUCell(h) for _ in range(3)]\n",
    "# rnn_cell_fw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pos2int = {\"CC\":0, \"CD\":1, \"DT\":2, \"EX\":3, \"FW\":4, \"IN\":5, \"JJ\":6, \"JJR\":7, \"JJS\":8, \\\n",
    "# \"LS\":9, \"MD\":10, \"NN\":11, \"NNS\":12, \"NNP\":13, \"NNPS\":14, \"PDT\":15, \"POS\":16, \\\n",
    "# \"PRP\":17, \"PRP$\":18, \"RB\":19, \"RBR\":20, \"RBS\":21, \"RP\":22, \"SYM\":23, \"TO\":24, \\\n",
    "# \"UH\":25, \"VB\":26, \"VBD\":27, \"VBG\":28, \"VBN\":29, \"VBP\":30, \"VBZ\":31, \"WDT\":32, \\\n",
    "# \"WP\":33, \"WP$\":34, \"WRB\":35}\n",
    "\n",
    "# ner2int_simple = {\"\":0,\"FACILITY\":1, \"GPE\":2, \"GSP\":3, \"LOCATION\":4, \"ORGANIZATION\":5, \"PERSON\":6}\n",
    "\n",
    "# s = \"Nancy Smith went to the United States last year and attended McDonalds at 3 o'clock\"\n",
    "# #s = \"bskyb launched its hdtv service , sky+ hd , on 22 may 2006 . prior to its launch , bskyb claimed that 40,000 people had registered to receive the hd service \"\n",
    "\n",
    "# s = s.split()\n",
    "\n",
    "# print(ne_chunk(pos_tag(s)))\n",
    "# print\n",
    "\n",
    "# pos = [pos2int[x[1]] for x in pos_tag(s)]\n",
    "# print(pos)\n",
    "\n",
    "# ner = [ner2int_simple[str(x[2])[2:]] for x in tree2conlltags(ne_chunk(pos_tag(s)))]\n",
    "# print(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def split_by_whitespace(sentence):\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(re.split(\" \", space_separated_fragment))\n",
    "    return [w for w in words if w]\n",
    "\n",
    "def sentence_to_token_ids(sentence):\n",
    "    \"\"\"Turns an already-tokenized sentence string into word indices\n",
    "    e.g. \"i do n't know\" -> [9, 32, 16, 96]\n",
    "    Note any token that isn't in the word2id mapping gets mapped to the id for UNK\n",
    "    \"\"\"\n",
    "    tokens = split_by_whitespace(sentence) # list of strings\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context_line = \"from the 9th century , the pecheneg nomads and America began an uneasy relationship with kievan rus' . for over two centuries they launched sporadic raids into the lands of rus' , which sometimes escalated into full-scale wars ( such as the 920 war on the pechenegs by igor of kiev reported in the primary chronicle ) , but there were also temporary military alliances ( e.g . the 943 byzantine campaign by igor ) . in 968 , the pechenegs attacked and besieged the city of kiev . some speculation exists that the pechenegs drove off the tivertsi and the ulichs to the regions of the upper dniester river in bukovina . the byzantine empire was known to support the pechenegs in their military campaigns against the eastern slavic states . [ citation needed ]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "# context_tokens = sentence_to_token_ids(context_line)\n",
    "\n",
    "# pos2int = {\"CC\":0, \"CD\":1, \"DT\":2, \"EX\":3, \"FW\":4, \"IN\":5, \"JJ\":6, \"JJR\":7, \"JJS\":8, \\\n",
    "#     \"LS\":9, \"MD\":10, \"NN\":11, \"NNS\":12, \"NNP\":13, \"NNPS\":14, \"PDT\":15, \"POS\":16, \\\n",
    "#     \"PRP\":17, \"PRP$\":18, \"RB\":19, \"RBR\":20, \"RBS\":21, \"RP\":22, \"SYM\":23, \"TO\":24, \\\n",
    "#     \"UH\":25, \"VB\":26, \"VBD\":27, \"VBG\":28, \"VBN\":29, \"VBP\":30, \"VBZ\":31, \"WDT\":32, \\\n",
    "#     \"WP\":33, \"WP$\":34, \"WRB\":35}\n",
    "\n",
    "# ner2int = {\"O\":0, \"PERSON\":1, \"LOCATION\":2, \"ORGANIZATION\":3, \"MISC\":4, \"MONEY\":5, \\\n",
    "#            \"NUMBER\":6, \"ORDINAL\":7, \"PERCENT\":8, \"DATE\":9, \"TIME\":10, \"DURATION\":11, \"SET\":12, \\\n",
    "#            \"EMAIL\":13, \"URL\":14, \"CITY\":15, \"STATE_OR_PROVINCE\":16, \"COUNTRY\":17, \"NATIONALITY\":18, \\\n",
    "#            \"RELIGION\":19, \"TITLE\":20, \"IDEOLOGY\":21, \"CRIMINAL_CHARGE\":22, \"CAUSE_OF_DEATH\":23}\n",
    "\n",
    "# output = nlp.annotate(context_line, properties={\n",
    "#     'annotators': 'pos,ner',\n",
    "#     'tokenize.language': 'Whitespace',\n",
    "#     'outputFormat': 'json'})\n",
    "# ner_tags = np.array([ner2int[str(tok['ner'])] if str(tok['ner']) in ner2int.keys() else 0 for s in output['sentences'] for tok in s['tokens']])\n",
    "# pos_tags = np.array([pos2int[str(tok['pos'])] if str(tok['pos']) in pos2int.keys() else -1 for s in output['sentences'] for tok in s['tokens']])\n",
    "# lems     = [tok['lemma'] for s in output['sentences'] for tok in s['tokens']]\n",
    "# assert (len(ner_tags)==len(context_tokens)), \"%d, %d, %s\" % (len(ner_tags), len(context_tokens), context_line)\n",
    "# assert (len(pos_tags)==len(context_tokens)), \"%d, %d\" % (len(pos_tags), len(context_tokens))\n",
    "# assert (len(lems)==len(context_tokens)),     \"%d, %d\" % (len(lems),     len(context_tokens))\n",
    "\n",
    "\n",
    "# print(context_line)\n",
    "# print(ner_tags)\n",
    "# print(pos_tags)\n",
    "# print(len(ner_tags))\n",
    "# print(len(pos_tags))\n",
    "# print(len(sentence_to_token_ids(context_line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# context_words = ['This','is','a','BART']\n",
    "# context_words_lem = ['this','is','a','bart']\n",
    "# query_words  = ['foo','bar','this']\n",
    "\n",
    "# a = np.array([int(any(context_word==q for q in query_words)) for context_word,context_word_lem in zip(context_words,context_words_lem)]) # original form\n",
    "# b = np.array([int(any(context_word.lower()==q for q in query_words)) for context_word,context_word_lem in zip(context_words,context_words_lem)]) # lower case\n",
    "# c = np.array([int(any(context_word_lem==q for q in query_words)) for context_word,context_word_lem in zip(context_words,context_words_lem)],dtype=np.float32)  # lower case\n",
    "\n",
    "# print(a)\n",
    "# print(b)\n",
    "# print(c)\n",
    "\n",
    "# f = np.concatenate([np.expand_dims(x,1) for x in (a,b,c)],axis=1)\n",
    "# f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'', u'', u'', u'', u'', u'', u'', u'', u'', u'GPE', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'']\n",
      "['IN', 'DT', 'CD', 'NN', ',', 'DT', 'NN', 'NNS', 'CC', 'NNP', 'VBD', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NN', '.', 'IN', 'IN', 'CD', 'NNS', 'PRP', 'VBD', 'JJ', 'NNS', 'IN', 'DT', 'NNS', 'IN', 'NN', ',', 'WDT', 'RB', 'VBD', 'IN', 'JJ', 'NNS', '(', 'JJ', 'IN', 'DT', 'CD', 'NN', 'IN', 'DT', 'NN', 'IN', 'NN', 'IN', 'NN', 'VBN', 'IN', 'DT', 'JJ', 'NN', ')', ',', 'CC', 'EX', 'VBD', 'RB', 'JJ', 'JJ', 'NNS', '(', 'NN', '.', 'DT', 'CD', 'NN', 'NN', 'IN', 'NN', ')', '.', 'IN', 'CD', ',', 'DT', 'NN', 'VBD', 'CC', 'VBD', 'DT', 'NN', 'IN', 'NN', '.', 'DT', 'NN', 'VBZ', 'IN', 'DT', 'NNS', 'VBD', 'RP', 'DT', 'NN', 'CC', 'DT', 'NN', 'TO', 'DT', 'NNS', 'IN', 'DT', 'JJ', 'NN', 'NN', 'IN', 'NN', '.', 'DT', 'NN', 'NN', 'VBD', 'VBN', 'TO', 'VB', 'DT', 'NN', 'IN', 'PRP$', 'JJ', 'NNS', 'IN', 'DT', 'JJ', 'JJ', 'NNS', '.', 'VB', 'NN', 'VBN', 'NN']\n",
      "['from', 'the', '9th', 'century', ',', 'the', 'pecheneg', 'nomad', 'and', 'America', 'begin', 'an', 'uneasy', 'relationship', 'with', 'kievan', \"rus'\", '.', 'for', 'over', 'two', 'century', 'they', 'launch', 'sporadic', 'raid', 'into', 'the', 'land', 'of', \"rus'\", ',', 'which', 'sometimes', 'escalate', 'into', 'full-scale', 'war', '(', 'such', 'a', 'the', '920', 'war', 'on', 'the', 'pechenegs', 'by', 'igor', 'of', 'kiev', 'report', 'in', 'the', 'primary', 'chronicle', ')', ',', 'but', 'there', 'be', 'also', 'temporary', 'military', 'alliance', '(', 'e.g', '.', 'the', '943', 'byzantine', 'campaign', 'by', 'igor', ')', '.', 'in', '968', ',', 'the', 'pechenegs', 'attack', 'and', 'besiege', 'the', 'city', 'of', 'kiev', '.', 'some', 'speculation', 'exist', 'that', 'the', 'pechenegs', 'drive', 'off', 'the', 'tivertsi', 'and', 'the', 'ulichs', 'to', 'the', 'region', 'of', 'the', 'upper', 'dniester', 'river', 'in', 'bukovina', '.', 'the', 'byzantine', 'empire', 'be', 'know', 'to', 'support', 'the', 'pechenegs', 'in', 'their', 'military', 'campaign', 'against', 'the', 'eastern', 'slavic', 'state', '.', '[', 'citation', 'need', ']']\n",
      "136\n",
      "136\n",
      "136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_tokens = split_by_whitespace(context_line)\n",
    "pos_tree = pos_tag(context_tokens)\n",
    "ne_tree = ne_chunk(pos_tree)\n",
    "tree2conlltags(ne_tree)\n",
    "\n",
    "pos2int = {\"CC\":0, \"CD\":1, \"DT\":2, \"EX\":3, \"FW\":4, \"IN\":5, \"JJ\":6, \"JJR\":7, \"JJS\":8, \\\n",
    "    \"LS\":9, \"MD\":10, \"NN\":11, \"NNS\":12, \"NNP\":13, \"NNPS\":14, \"PDT\":15, \"POS\":16, \\\n",
    "    \"PRP\":17, \"PRP$\":18, \"RB\":19, \"RBR\":20, \"RBS\":21, \"RP\":22, \"SYM\":23, \"TO\":24, \\\n",
    "    \"UH\":25, \"VB\":26, \"VBD\":27, \"VBG\":28, \"VBN\":29, \"VBP\":30, \"VBZ\":31, \"WDT\":32, \\\n",
    "    \"WP\":33, \"WP$\":34, \"WRB\":35}\n",
    "\n",
    "ner2int = {\"O\":0, \"PERSON\":1, \"LOCATION\":2, \"ORGANIZATION\":3, \"GSP\":4, \"GPE\":5, \"FACILITY\":6}\n",
    "\n",
    "pos_keys = pos2int.keys()\n",
    "ner_keys = ner2int.keys() \n",
    "\n",
    "pos_tags = [p[1] for p in pos_tag(context_tokens)]\n",
    "ner_tags = [ne[2][2:] for ne in tree2conlltags(ne_chunk(pos_tree))]\n",
    "\n",
    "pos_ids = [pos2int[pos] if pos in pos_keys else -1 for pos in pos_tags]\n",
    "ner_ids = [ner2int[ne]  if ne  in ner_keys else 0  for ne  in ner_tags]\n",
    "            \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lems = [str(lemmatizer.lemmatize(tok,get_wordnet_pos(pos))) if get_wordnet_pos(pos) else str(lemmatizer.lemmatize(tok)) for tok,pos in zip(context_tokens,pos_tags)]\n",
    "\n",
    "print(ner_tags)\n",
    "print(pos_tags)\n",
    "print(lems)\n",
    "\n",
    "print(len(ner_ids))\n",
    "print(len(pos_ids))\n",
    "print(len(lems))\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from nltk.tag import StanfordNERTagger\n",
    "# import time\n",
    "\n",
    "# st = StanfordNERTagger('/Users/andrewweitz/_Stanford/year6/cs224n/stanford-ner-2018-02-27/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "#     '/Users/andrewweitz/_Stanford/year6/cs224n/stanford-ner-2018-02-27/stanford-ner.jar',\n",
    "#     encoding='utf-8')\n",
    "\n",
    "# tic = time.time()\n",
    "# classified_text = st.tag(context_tokens)\n",
    "# print(time.time()-tic)\n",
    "# print(classified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sner import Ner\n",
    "\n",
    "# tagger = Ner(host='localhost',port=9199)\n",
    "\n",
    "# tic = time.time()\n",
    "# t = tagger.get_entities(context_line)\n",
    "# print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.tag.perceptron import PerceptronTagger\n",
    "# tagger = PerceptronTagger()\n",
    "# tagset = None\n",
    "# tic = time.time()\n",
    "# tags = nltk.tag._pos_tag(context_tokens, tagset, tagger)\n",
    "# print(time.time()-tic)\n",
    "# tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = pickle.load( open( \"../output.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vocab import PAD_ID, UNK_ID\n",
    "\n",
    "def padded2(token_batch, num_feats, batch_pad=0, ):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      token_batch: List (length batch size) of arrays.\n",
    "      batch_pad: Int. Length to pad to. If 0, pad to maximum length sequence in token_batch.\n",
    "    Returns:\n",
    "      List (length batch_size) of padded of lists of ints.\n",
    "        All are same length - batch_pad if batch_pad!=0, otherwise the maximum length in token_batch\n",
    "    \"\"\"\n",
    "    maxlen = max(map(lambda x: len(x), token_batch)) if batch_pad == 0 else batch_pad\n",
    "    return map(lambda token_list: token_list + [num_feats*(PAD_ID,)] * (maxlen - len(token_list)) , token_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000732898712158\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "z = padded2(x,4,600)\n",
    "print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 600, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(z)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = x[0]\n",
    "len(f)\n",
    "f = f[:130]\n",
    "len(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

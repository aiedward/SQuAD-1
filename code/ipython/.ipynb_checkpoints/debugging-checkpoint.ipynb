{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "import numpy as np\n",
    "# from nltk import pos_tag, ne_chunk\n",
    "# from nltk.chunk import tree2conlltags\n",
    "#from stanfordcorenlp import StanfordCoreNLP\n",
    "from pycorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = tf.constant(np.arange(0, 30, dtype=np.int32), shape=[2, 5, 3])\n",
    "# b = tf.constant(np.arange(0, 24, dtype=np.int32), shape=[2, 3, 4])\n",
    "# c = tf.matmul(a, b)\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = tf.Session()\n",
    "# print(sess.run(a[0]))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(b[0]))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(c))\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = tf.constant(np.arange(0, 30, dtype=np.float),shape=[2, 5, 3])\n",
    "# x2 = tf.nn.softmax(x,axis=2)\n",
    "\n",
    "# sess = tf.Session()\n",
    "# print(sess.run(x[0]))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(x2[0]))\n",
    "# print('~~~~~~~~')\n",
    "# print(np.sum(sess.run(x2[0]),axis=1))\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = tf.constant(np.arange(0, 10, dtype=np.float), shape=[2, 5, 1])\n",
    "# z = tf.constant(np.arange(0, 15, dtype=np.float), shape=[1, 5, 3])\n",
    "# w = tf.multiply(y,z)\n",
    "\n",
    "# sess = tf.Session()\n",
    "# print(sess.run(y))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(z))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(w))\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.random.permutation(24).reshape(2,3,4)\n",
    "# print(a[0])\n",
    "# print(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = np.transpose(np.asarray([np.unravel_index(np.argmax(x, axis=None), x.shape) for x in a]))\n",
    "# print(idx)\n",
    "# row_max = idx[0]\n",
    "# col_max = idx[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = 100\n",
    "# rnn_cell_fw = [rnn_cell.GRUCell(h) for _ in range(3)]\n",
    "# rnn_cell_fw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos2int = {\"CC\":0, \"CD\":1, \"DT\":2, \"EX\":3, \"FW\":4, \"IN\":5, \"JJ\":6, \"JJR\":7, \"JJS\":8, \\\n",
    "# \"LS\":9, \"MD\":10, \"NN\":11, \"NNS\":12, \"NNP\":13, \"NNPS\":14, \"PDT\":15, \"POS\":16, \\\n",
    "# \"PRP\":17, \"PRP$\":18, \"RB\":19, \"RBR\":20, \"RBS\":21, \"RP\":22, \"SYM\":23, \"TO\":24, \\\n",
    "# \"UH\":25, \"VB\":26, \"VBD\":27, \"VBG\":28, \"VBN\":29, \"VBP\":30, \"VBZ\":31, \"WDT\":32, \\\n",
    "# \"WP\":33, \"WP$\":34, \"WRB\":35}\n",
    "\n",
    "# ner2int_simple = {\"\":0,\"FACILITY\":1, \"GPE\":2, \"GSP\":3, \"LOCATION\":4, \"ORGANIZATION\":5, \"PERSON\":6}\n",
    "\n",
    "# s = \"Nancy Smith went to the United States last year and attended McDonalds at 3 o'clock\"\n",
    "# #s = \"bskyb launched its hdtv service , sky+ hd , on 22 may 2006 . prior to its launch , bskyb claimed that 40,000 people had registered to receive the hd service \"\n",
    "\n",
    "# s = s.split()\n",
    "\n",
    "# print(ne_chunk(pos_tag(s)))\n",
    "# print\n",
    "\n",
    "# pos = [pos2int[x[1]] for x in pos_tag(s)]\n",
    "# print(pos)\n",
    "\n",
    "# ner = [ner2int_simple[str(x[2])[2:]] for x in tree2conlltags(ne_chunk(pos_tag(s)))]\n",
    "# print(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"the epidermis is typically 10 to 30 cells thick ; its main function is to provide a waterproof layer . its outermost cells are constantly lost ; its bottommost cells are constantly dividing and pushing upward . the middle layer , the dermis , is 15 to 40 times thicker than the epidermis . the dermis is made up of many components , such as bony structures and blood vessels . the hypodermis is made up of adipose tissue . its job is to store lipids , and to provide cushioning and insulation . the thickness of this layer varies widely from species to species .\"\n",
    "output = nlp.annotate(text, properties={'annotators': 'pos,ner,lemma','outputFormat': 'json'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the epidermis is typically 10 to 30 cells thick ; its main function is to provide a waterproof layer . its outermost cells are constantly lost ; its bottommost cells are constantly dividing and pushing upward . the middle layer , the dermis , is 15 to 40 times thicker than the epidermis . the dermis is made up of many components , such as bony structures and blood vessels . the hypodermis is made up of adipose tissue . its job is to store lipids , and to provide cushioning and insulation . the thickness of this layer varies widely from species to species .\n",
      "[0, 0, 0, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 6, 0, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0]\n",
      "[2, 11, 31, 19, 1, 24, 1, 12, 6, -1, 18, 6, 11, 31, 24, 26, 2, 6, 11, -1, 18, 6, 12, 30, 19, 29, -1, 18, 11, 12, 30, 19, 28, 0, 28, 6, -1, 2, 6, 11, -1, 2, 11, -1, 31, 1, 24, 1, 12, 7, 5, 2, 11, -1, 2, 11, 31, 29, 22, 5, 6, 12, -1, 6, 5, 6, 12, 0, 11, 12, -1, 2, 11, 31, 29, 22, 5, 11, 11, -1, 18, 11, 31, 24, 26, 12, -1, 0, 24, 26, 11, 0, 11, -1, 2, 11, 5, 2, 11, 31, 19, 5, 12, 24, 12, -1]\n",
      "106\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "pos2int = {\"CC\":0, \"CD\":1, \"DT\":2, \"EX\":3, \"FW\":4, \"IN\":5, \"JJ\":6, \"JJR\":7, \"JJS\":8, \\\n",
    "\"LS\":9, \"MD\":10, \"NN\":11, \"NNS\":12, \"NNP\":13, \"NNPS\":14, \"PDT\":15, \"POS\":16, \\\n",
    "\"PRP\":17, \"PRP$\":18, \"RB\":19, \"RBR\":20, \"RBS\":21, \"RP\":22, \"SYM\":23, \"TO\":24, \\\n",
    "\"UH\":25, \"VB\":26, \"VBD\":27, \"VBG\":28, \"VBN\":29, \"VBP\":30, \"VBZ\":31, \"WDT\":32, \\\n",
    "\"WP\":33, \"WP$\":34, \"WRB\":35}\n",
    "\n",
    "ner2int = {\"O\":0, \"PERSON\":1, \"LOCATION\":2, \"ORGANIZATION\":3, \"MISC\":4, \"MONEY\":5, \\\n",
    "           \"NUMBER\":6, \"ORDINAL\":7, \"PERCENT\":8, \"DATE\":9, \"TIME\":10, \"DURATION\":11, \"SET\":12, \\\n",
    "           \"EMAIL\":13, \"URL\":14, \"CITY\":15, \"STATE_OR_PROVINCE\":16, \"COUNTRY\":17, \"NATIONALITY\":18, \\\n",
    "           \"RELIGION\":19, \"TITLE\":20, \"IDEOLOGY\":21, \"CRIMINAL_CHARGE\":22, \"CAUSE_OF_DEATH\":23}\n",
    "\n",
    "ner_tags = [ner2int[str(tok['ner'])] if str(tok['ner']) in ner2int.keys() else 0 for s in output['sentences'] for tok in s['tokens']]\n",
    "pos_tags = [pos2int[str(tok['pos'])] if str(tok['pos']) in pos2int.keys() else -1 for s in output['sentences'] for tok in s['tokens']]\n",
    "\n",
    "print(text)\n",
    "print(ner_tags)\n",
    "print(pos_tags)\n",
    "print(len(ner_tags))\n",
    "print(len(pos_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_words = ['This','is','a','BART']\n",
    "# context_words_lem = ['this','is','a','bart']\n",
    "# query_words  = ['foo','bar','this']\n",
    "\n",
    "# a = np.array([int(any(context_word==q for q in query_words)) for context_word,context_word_lem in zip(context_words,context_words_lem)]) # original form\n",
    "# b = np.array([int(any(context_word.lower()==q for q in query_words)) for context_word,context_word_lem in zip(context_words,context_words_lem)]) # lower case\n",
    "# c = np.array([int(any(context_word_lem==q for q in query_words)) for context_word,context_word_lem in zip(context_words,context_words_lem)],dtype=np.float32)  # lower case\n",
    "\n",
    "# print(a)\n",
    "# print(b)\n",
    "# print(c)\n",
    "\n",
    "# f = np.concatenate([np.expand_dims(x,1) for x in (a,b,c)],axis=1)\n",
    "# f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def split_by_whitespace(sentence):\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(re.split(\" \", space_separated_fragment))\n",
    "    return [w for w in words if w]\n",
    "\n",
    "def sentence_to_token_ids(sentence):\n",
    "    \"\"\"Turns an already-tokenized sentence string into word indices\n",
    "    e.g. \"i do n't know\" -> [9, 32, 16, 96]\n",
    "    Note any token that isn't in the word2id mapping gets mapped to the id for UNK\n",
    "    \"\"\"\n",
    "    tokens = split_by_whitespace(sentence) # list of strings\n",
    "    return tokens\n",
    "\n",
    "toksQA = sentence_to_token_ids(text)\n",
    "len(toksQA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_line = text\n",
    "context_tokens = sentence_to_token_ids(context_line)\n",
    "pos2int = {\"CC\":0, \"CD\":1, \"DT\":2, \"EX\":3, \"FW\":4, \"IN\":5, \"JJ\":6, \"JJR\":7, \"JJS\":8, \\\n",
    "    \"LS\":9, \"MD\":10, \"NN\":11, \"NNS\":12, \"NNP\":13, \"NNPS\":14, \"PDT\":15, \"POS\":16, \\\n",
    "    \"PRP\":17, \"PRP$\":18, \"RB\":19, \"RBR\":20, \"RBS\":21, \"RP\":22, \"SYM\":23, \"TO\":24, \\\n",
    "    \"UH\":25, \"VB\":26, \"VBD\":27, \"VBG\":28, \"VBN\":29, \"VBP\":30, \"VBZ\":31, \"WDT\":32, \\\n",
    "    \"WP\":33, \"WP$\":34, \"WRB\":35}\n",
    "\n",
    "ner2int = {\"O\":0, \"PERSON\":1, \"LOCATION\":2, \"ORGANIZATION\":3, \"MISC\":4, \"MONEY\":5, \\\n",
    "           \"NUMBER\":6, \"ORDINAL\":7, \"PERCENT\":8, \"DATE\":9, \"TIME\":10, \"DURATION\":11, \"SET\":12, \\\n",
    "           \"EMAIL\":13, \"URL\":14, \"CITY\":15, \"STATE_OR_PROVINCE\":16, \"COUNTRY\":17, \"NATIONALITY\":18, \\\n",
    "           \"RELIGION\":19, \"TITLE\":20, \"IDEOLOGY\":21, \"CRIMINAL_CHARGE\":22, \"CAUSE_OF_DEATH\":23}\n",
    "\n",
    "output = nlp.annotate(context_line, properties={'annotators': 'pos,ner','outputFormat': 'json'})\n",
    "ner_tags = np.array([ner2int[str(tok['ner'])] if str(tok['ner']) in ner2int.keys() else 0 for s in output['sentences'] for tok in s['tokens']])\n",
    "pos_tags = np.array([pos2int[str(tok['pos'])] if str(tok['pos']) in pos2int.keys() else -1 for s in output['sentences'] for tok in s['tokens']])\n",
    "lems     = [tok['lemma'] for s in output['sentences'] for tok in s['tokens']]\n",
    "assert (len(ner_tags)==len(context_tokens)), \"%d, %d, %s\" % (len(ner_tags), len(context_tokens), context_line)\n",
    "assert (len(pos_tags)==len(context_tokens)), \"%d, %d\" % (len(pos_tags), len(context_tokens))\n",
    "assert (len(lems)==len(context_tokens)),     \"%d, %d\" % (len(lems),     len(context_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

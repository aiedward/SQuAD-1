{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "import numpy as np\n",
    "from nltk import pos_tag, ne_chunk, FreqDist\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import time\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = tf.constant(np.arange(0, 30, dtype=np.int32), shape=[2, 5, 3])\n",
    "# b = tf.constant(np.arange(0, 24, dtype=np.int32), shape=[2, 3, 4])\n",
    "# c = tf.matmul(a, b)\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sess = tf.Session()\n",
    "# print(sess.run(a[0]))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(b[0]))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(c))\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x = tf.constant(np.arange(0, 30, dtype=np.float),shape=[2, 5, 3])\n",
    "# x2 = tf.nn.softmax(x,axis=2)\n",
    "\n",
    "# sess = tf.Session()\n",
    "# print(sess.run(x[0]))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(x2[0]))\n",
    "# print('~~~~~~~~')\n",
    "# print(np.sum(sess.run(x2[0]),axis=1))\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y = tf.constant(np.arange(0, 10, dtype=np.float), shape=[2, 5, 1])\n",
    "# z = tf.constant(np.arange(0, 15, dtype=np.float), shape=[1, 5, 3])\n",
    "# w = tf.multiply(y,z)\n",
    "\n",
    "# sess = tf.Session()\n",
    "# print(sess.run(y))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(z))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(w))\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = np.random.permutation(24).reshape(2,3,4)\n",
    "# print(a[0])\n",
    "# print(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# idx = np.transpose(np.asarray([np.unravel_index(np.argmax(x, axis=None), x.shape) for x in a]))\n",
    "# print(idx)\n",
    "# row_max = idx[0]\n",
    "# col_max = idx[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# h = 100\n",
    "# rnn_cell_fw = [rnn_cell.GRUCell(h) for _ in range(3)]\n",
    "# rnn_cell_fw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pos2int = {\"CC\":0, \"CD\":1, \"DT\":2, \"EX\":3, \"FW\":4, \"IN\":5, \"JJ\":6, \"JJR\":7, \"JJS\":8, \\\n",
    "# \"LS\":9, \"MD\":10, \"NN\":11, \"NNS\":12, \"NNP\":13, \"NNPS\":14, \"PDT\":15, \"POS\":16, \\\n",
    "# \"PRP\":17, \"PRP$\":18, \"RB\":19, \"RBR\":20, \"RBS\":21, \"RP\":22, \"SYM\":23, \"TO\":24, \\\n",
    "# \"UH\":25, \"VB\":26, \"VBD\":27, \"VBG\":28, \"VBN\":29, \"VBP\":30, \"VBZ\":31, \"WDT\":32, \\\n",
    "# \"WP\":33, \"WP$\":34, \"WRB\":35}\n",
    "\n",
    "# ner2int_simple = {\"\":0,\"FACILITY\":1, \"GPE\":2, \"GSP\":3, \"LOCATION\":4, \"ORGANIZATION\":5, \"PERSON\":6}\n",
    "\n",
    "# s = \"Nancy Smith went to the United States last year and attended McDonalds at 3 o'clock\"\n",
    "# #s = \"bskyb launched its hdtv service , sky+ hd , on 22 may 2006 . prior to its launch , bskyb claimed that 40,000 people had registered to receive the hd service \"\n",
    "\n",
    "# s = s.split()\n",
    "\n",
    "# print(ne_chunk(pos_tag(s)))\n",
    "# print\n",
    "\n",
    "# pos = [pos2int[x[1]] for x in pos_tag(s)]\n",
    "# print(pos)\n",
    "\n",
    "# ner = [ner2int_simple[str(x[2])[2:]] for x in tree2conlltags(ne_chunk(pos_tag(s)))]\n",
    "# print(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# def split_by_whitespace(sentence):\n",
    "#     words = []\n",
    "#     for space_separated_fragment in sentence.strip().split():\n",
    "#         words.extend(re.split(\" \", space_separated_fragment))\n",
    "#     return [w for w in words if w]\n",
    "\n",
    "# def sentence_to_token_ids(sentence):\n",
    "#     \"\"\"Turns an already-tokenized sentence string into word indices\n",
    "#     e.g. \"i do n't know\" -> [9, 32, 16, 96]\n",
    "#     Note any token that isn't in the word2id mapping gets mapped to the id for UNK\n",
    "#     \"\"\"\n",
    "#     tokens = split_by_whitespace(sentence) # list of strings\n",
    "#     return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context_line = \"the service started on 1 september 1993 based on the idea from the then chief executive officer , sam chisholm and rupert murdoch , of converting the company business strategy to an entirely fee-based concept . the new package included four channels formerly available free-to-air , broadcasting on astra 's satellites , as well as introducing new channels . the service continued until the closure of bskyb 's analogue service on 27 september 2001 , due to the launch and expansion of the sky digital platform . some of the channels did broadcast either in the clear or soft encrypted ( whereby a videocrypt decoder was required to decode , without a subscription card ) prior to their addition to the sky multichannels package . within two months of the launch , bskyb gained 400,000 new subscribers , with the majority taking at least one premium channel as well , which helped bskyb reach 3.5 million households by mid-1994 . michael grade criticized the operations in front of the select committee on national heritage , mainly for the lack of original programming on many of the new channels .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "# context_tokens = sentence_to_token_ids(context_line)\n",
    "\n",
    "# pos2int = {\"CC\":0, \"CD\":1, \"DT\":2, \"EX\":3, \"FW\":4, \"IN\":5, \"JJ\":6, \"JJR\":7, \"JJS\":8, \\\n",
    "#     \"LS\":9, \"MD\":10, \"NN\":11, \"NNS\":12, \"NNP\":13, \"NNPS\":14, \"PDT\":15, \"POS\":16, \\\n",
    "#     \"PRP\":17, \"PRP$\":18, \"RB\":19, \"RBR\":20, \"RBS\":21, \"RP\":22, \"SYM\":23, \"TO\":24, \\\n",
    "#     \"UH\":25, \"VB\":26, \"VBD\":27, \"VBG\":28, \"VBN\":29, \"VBP\":30, \"VBZ\":31, \"WDT\":32, \\\n",
    "#     \"WP\":33, \"WP$\":34, \"WRB\":35}\n",
    "\n",
    "# ner2int = {\"O\":0, \"PERSON\":1, \"LOCATION\":2, \"ORGANIZATION\":3, \"MISC\":4, \"MONEY\":5, \\\n",
    "#            \"NUMBER\":6, \"ORDINAL\":7, \"PERCENT\":8, \"DATE\":9, \"TIME\":10, \"DURATION\":11, \"SET\":12, \\\n",
    "#            \"EMAIL\":13, \"URL\":14, \"CITY\":15, \"STATE_OR_PROVINCE\":16, \"COUNTRY\":17, \"NATIONALITY\":18, \\\n",
    "#            \"RELIGION\":19, \"TITLE\":20, \"IDEOLOGY\":21, \"CRIMINAL_CHARGE\":22, \"CAUSE_OF_DEATH\":23}\n",
    "\n",
    "# output = nlp.annotate(context_line, properties={\n",
    "#     'annotators': 'pos,ner',\n",
    "#     'tokenize.language': 'Whitespace',\n",
    "#     'outputFormat': 'json'})\n",
    "# ner_tags = np.array([ner2int[str(tok['ner'])] if str(tok['ner']) in ner2int.keys() else 0 for s in output['sentences'] for tok in s['tokens']])\n",
    "# pos_tags = np.array([pos2int[str(tok['pos'])] if str(tok['pos']) in pos2int.keys() else -1 for s in output['sentences'] for tok in s['tokens']])\n",
    "# lems     = [tok['lemma'] for s in output['sentences'] for tok in s['tokens']]\n",
    "# assert (len(ner_tags)==len(context_tokens)), \"%d, %d, %s\" % (len(ner_tags), len(context_tokens), context_line)\n",
    "# assert (len(pos_tags)==len(context_tokens)), \"%d, %d\" % (len(pos_tags), len(context_tokens))\n",
    "# assert (len(lems)==len(context_tokens)),     \"%d, %d\" % (len(lems),     len(context_tokens))\n",
    "\n",
    "\n",
    "# print(context_line)\n",
    "# print(ner_tags)\n",
    "# print(pos_tags)\n",
    "# print(len(ner_tags))\n",
    "# print(len(pos_tags))\n",
    "# print(len(sentence_to_token_ids(context_line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0]\n",
      "[0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "context_tokens = ['this','is','a','bart']\n",
    "lems           = ['this','is','a','bar']\n",
    "qn_tokens  = ['foo','bar','this','this','a']\n",
    "\n",
    "match_orig  = [int(sum([context_token==q for q in qn_tokens])==1)     for context_token     in context_tokens] # original form\n",
    "match_lemma = [int(sum([context_token_lem==q for q in qn_tokens])==1) for context_token_lem in lems]    # lemma form\n",
    "\n",
    "print(match_orig)\n",
    "print(match_lemma)\n",
    "\n",
    "# f = np.concatenate([np.expand_dims(x,1) for x in (a,b,c)],axis=1)\n",
    "# f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_token = context_tokens[0]\n",
    "int(sum([context_token==q for q in qn_tokens])==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "#     if treebank_tag.startswith('J'):\n",
    "#         return wordnet.ADJ\n",
    "#     elif treebank_tag.startswith('V'):\n",
    "#         return wordnet.VERB\n",
    "#     elif treebank_tag.startswith('N'):\n",
    "#         return wordnet.NOUN\n",
    "#     elif treebank_tag.startswith('R'):\n",
    "#         return wordnet.ADV\n",
    "#     else:\n",
    "#         return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# context_tokens = split_by_whitespace(context_line)\n",
    "# pos_tree = pos_tag(context_tokens)\n",
    "# ne_tree = ne_chunk(pos_tree)\n",
    "# tree2conlltags(ne_tree)\n",
    "\n",
    "# pos2int = {\"CC\":0, \"CD\":1, \"DT\":2, \"EX\":3, \"FW\":4, \"IN\":5, \"JJ\":6, \"JJR\":7, \"JJS\":8, \\\n",
    "#     \"LS\":9, \"MD\":10, \"NN\":11, \"NNS\":12, \"NNP\":13, \"NNPS\":14, \"PDT\":15, \"POS\":16, \\\n",
    "#     \"PRP\":17, \"PRP$\":18, \"RB\":19, \"RBR\":20, \"RBS\":21, \"RP\":22, \"SYM\":23, \"TO\":24, \\\n",
    "#     \"UH\":25, \"VB\":26, \"VBD\":27, \"VBG\":28, \"VBN\":29, \"VBP\":30, \"VBZ\":31, \"WDT\":32, \\\n",
    "#     \"WP\":33, \"WP$\":34, \"WRB\":35}\n",
    "\n",
    "# ner2int = {\"O\":0, \"PERSON\":1, \"LOCATION\":2, \"ORGANIZATION\":3, \"GSP\":4, \"GPE\":5, \"FACILITY\":6}\n",
    "\n",
    "# pos_keys = pos2int.keys()\n",
    "# ner_keys = ner2int.keys() \n",
    "\n",
    "# pos_tags = [p[1] for p in pos_tag(context_tokens)]\n",
    "# ner_tags = [ne[2][2:] for ne in tree2conlltags(ne_chunk(pos_tree))]\n",
    "\n",
    "# pos_ids = [pos2int[pos] if pos in pos_keys else -1 for pos in pos_tags]\n",
    "# ner_ids = [ner2int[ne]  if ne  in ner_keys else 0  for ne  in ner_tags]\n",
    "            \n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# lems = [str(lemmatizer.lemmatize(tok,get_wordnet_pos(pos))) if get_wordnet_pos(pos) else str(lemmatizer.lemmatize(tok)) for tok,pos in zip(context_tokens,pos_tags)]\n",
    "\n",
    "# print(ner_ids)\n",
    "# print(pos_tags)\n",
    "# print(lems)\n",
    "\n",
    "# print(len(ner_ids))\n",
    "# print(len(pos_ids))\n",
    "# print(len(lems))\n",
    "# print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from nltk.tag import StanfordNERTagger\n",
    "# import time\n",
    "\n",
    "# st = StanfordNERTagger('/Users/andrewweitz/_Stanford/year6/cs224n/stanford-ner-2018-02-27/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "#     '/Users/andrewweitz/_Stanford/year6/cs224n/stanford-ner-2018-02-27/stanford-ner.jar',\n",
    "#     encoding='utf-8')\n",
    "\n",
    "# tic = time.time()\n",
    "# classified_text = st.tag(context_tokens)\n",
    "# print(time.time()-tic)\n",
    "# print(classified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sner import Ner\n",
    "\n",
    "# tagger = Ner(host='localhost',port=9199)\n",
    "\n",
    "# tic = time.time()\n",
    "# t = tagger.get_entities(context_line)\n",
    "# print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.tag.perceptron import PerceptronTagger\n",
    "# tagger = PerceptronTagger()\n",
    "# tagset = None\n",
    "# tic = time.time()\n",
    "# tags = nltk.tag._pos_tag(context_tokens, tagset, tagger)\n",
    "# print(time.time()-tic)\n",
    "# tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x = pickle.load( open( \"../output.p\", \"rb\" ) )\n",
    "\n",
    "# from vocab import PAD_ID, UNK_ID\n",
    "# def padded2(token_batch, num_feats, batch_pad=0, ):\n",
    "#     \"\"\"\n",
    "#     Inputs:\n",
    "#       token_batch: List (length batch size) of arrays.\n",
    "#       batch_pad: Int. Length to pad to. If 0, pad to maximum length sequence in token_batch.\n",
    "#     Returns:\n",
    "#       List (length batch_size) of padded of lists of ints.\n",
    "#         All are same length - batch_pad if batch_pad!=0, otherwise the maximum length in token_batch\n",
    "#     \"\"\"\n",
    "#     maxlen = max(map(lambda x: len(x), token_batch)) if batch_pad == 0 else batch_pad\n",
    "#     return map(lambda token_list: token_list + [num_feats*[PAD_ID,]] * (maxlen - len(token_list)) , token_batch)\n",
    "\n",
    "# tic = time.time()\n",
    "# z = padded2(x,4,600)\n",
    "# print(time.time()-tic)\n",
    "\n",
    "# y = np.array(z)\n",
    "# y.shape\n",
    "\n",
    "# f = x[0]\n",
    "# print(len(f))\n",
    "# f = f[:130]\n",
    "# print(len(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = 0.4\n",
    "# tic = time.time()\n",
    "# fdist = FreqDist(context_tokens)\n",
    "# tf = [a + (1-a)*fdist[w]/float(max(fdist.values())) for w in context_tokens]\n",
    "# print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# char2id = {\"a\":2, \"b\":3, \"c\":4, \"d\":5, \"e\":6, \"f\":7, \"g\":8, \\\n",
    "#     \"h\":9, \"i\":10, \"j\":11, \"k\":12, \"l\":13, \"m\":14, \"n\":15, \"o\":16, \\\n",
    "#     \"p\":17, \"q\":18, \"r\":19, \"s\":20, \"t\":21, \"u\":22, \"v\":23, \"w\":24, \\\n",
    "#     \"x\":25, \"y\":26, \"z\":27, \"0\":28, \"1\":29, \"2\":30, \"3\":31, \"4\":32, \\\n",
    "#     \"5\":33, \"6\":34, \"7\":35, \"8\":36, \"9\":37, \".\":38, \",\":39, '\"':40, \\\n",
    "#     \"?\":41, \"'\":42}\n",
    "    \n",
    "# tokens = [\"This\",\"is\",\"a\",\"WORD2?foo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word_len = 8\n",
    "# context_len = 15\n",
    "\n",
    "# tic = time.time()\n",
    "# charIDs = [[char2id[char] if char in char2id.keys() else -1 for char in tok.lower()] for tok  in tokens ]\n",
    "# print(time.time()-tic)\n",
    "\n",
    "# tic = time.time()\n",
    "# charIDs = [[ord(char) for char in tok.lower()] for tok  in tokens ]\n",
    "# print(time.time()-tic)\n",
    "\n",
    "# charIDs = [x[:word_len] for x in charIDs] # truncate to word_len\n",
    "# charIDs = padded(charIDs,word_len)\n",
    "# charIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PAD_ID = 0\n",
    "# UNK_ID = 1\n",
    "# def padded(token_batch, batch_pad=0):\n",
    "#     maxlen = max(map(lambda x: len(x), token_batch)) if batch_pad == 0 else batch_pad\n",
    "#     return map(lambda token_list: token_list + [PAD_ID] * (maxlen - len(token_list)), token_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# char_ids = pickle.load( open( \"../char_ids.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qn_ids = [[1,29,42,20,64,100,42,399],[400,33,3,33,42,10,10,399]]\n",
    "mcids = [29, 42, 400]\n",
    "mcids_dict = dict(zip(mcids,range(len(mcids))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False, True, True, False, False, False, True, False], [True, False, False, False, True, False, False, False]]\n",
      "[[0, 0, 1, 0, 0, 0, 1, 0], [2, 0, 0, 0, 1, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "mask = [[x in mcids_dict.keys() for x in qn_id ] for qn_id in qn_ids]\n",
    "emb_indices =  [[mcids_dict.get(x,0) for x in qn_id] for qn_id in qn_ids]\n",
    "\n",
    "print(mask)\n",
    "print(emb_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(4, 3)\n",
      "(2, 2, 3)\n",
      "[[[-10 -20 -30]\n",
      "  [ -4  -5  -6]]\n",
      "\n",
      " [[  1   2   3]\n",
      "  [ 40  50  60]]]\n"
     ]
    }
   ],
   "source": [
    "emb         = tf.constant([[[-1,-2,-3], [-4,-5,-6]], [[1,2,3], [4,5,6]]])\n",
    "emb_common  = tf.constant([[[-10,-20,-30], [-40,-50,-60]], [[10,20,30], [40,50,60]]])\n",
    "\n",
    "npmask = np.array([[1,0],[0,1]])\n",
    "mask = tf.constant(npmask,dtype=tf.bool)\n",
    "\n",
    "mask = tf.reshape(mask,[-1,])\n",
    "emb  = tf.reshape(emb,[-1,3])\n",
    "emb_common = tf.reshape(emb_common,[-1,3])\n",
    "\n",
    "print(mask.get_shape())\n",
    "print(emb.get_shape())\n",
    "print(emb_common.get_shape())\n",
    "\n",
    "output = tf.where(mask,emb_common,emb)\n",
    "print(output.get_shape())\n",
    "output = tf.reshape(output,[-1,2,3])\n",
    "print(output.get_shape())\n",
    "\n",
    "sess = tf.Session()\n",
    "print(sess.run(output))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [False,True,False]\n",
    "np.array(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0  1]\n",
      " [-1  2  5]]\n"
     ]
    }
   ],
   "source": [
    "a  = tf.constant([[11,22,33],[1,0,1],[4,4,4],[-1,2,5]])\n",
    "b  = tf.gather(a,[1,3])\n",
    "\n",
    "sess = tf.Session()\n",
    "print(sess.run(b))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

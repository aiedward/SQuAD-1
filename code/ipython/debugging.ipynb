{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "import numpy as np\n",
    "from nltk import pos_tag, ne_chunk, FreqDist\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import time\n",
    "\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = tf.constant(np.arange(0, 30, dtype=np.int32), shape=[2, 5, 3])\n",
    "# b = tf.constant(np.arange(0, 24, dtype=np.int32), shape=[2, 3, 4])\n",
    "# c = tf.matmul(a, b)\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sess = tf.Session()\n",
    "# print(sess.run(a[0]))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(b[0]))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(c))\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# x = tf.constant(np.arange(0, 30, dtype=np.float),shape=[2, 5, 3])\n",
    "# x2 = tf.nn.softmax(x,axis=2)\n",
    "\n",
    "# sess = tf.Session()\n",
    "# print(sess.run(x[0]))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(x2[0]))\n",
    "# print('~~~~~~~~')\n",
    "# print(np.sum(sess.run(x2[0]),axis=1))\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# y = tf.constant(np.arange(0, 10, dtype=np.float), shape=[2, 5, 1])\n",
    "# z = tf.constant(np.arange(0, 15, dtype=np.float), shape=[1, 5, 3])\n",
    "# w = tf.multiply(y,z)\n",
    "\n",
    "# sess = tf.Session()\n",
    "# print(sess.run(y))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(z))\n",
    "# print('~~~~~~~~')\n",
    "# print(sess.run(w))\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a = np.random.permutation(24).reshape(2,3,4)\n",
    "# print(a[0])\n",
    "# print(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# idx = np.transpose(np.asarray([np.unravel_index(np.argmax(x, axis=None), x.shape) for x in a]))\n",
    "# print(idx)\n",
    "# row_max = idx[0]\n",
    "# col_max = idx[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# h = 100\n",
    "# rnn_cell_fw = [rnn_cell.GRUCell(h) for _ in range(3)]\n",
    "# rnn_cell_fw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pos2int = {\"CC\":0, \"CD\":1, \"DT\":2, \"EX\":3, \"FW\":4, \"IN\":5, \"JJ\":6, \"JJR\":7, \"JJS\":8, \\\n",
    "# \"LS\":9, \"MD\":10, \"NN\":11, \"NNS\":12, \"NNP\":13, \"NNPS\":14, \"PDT\":15, \"POS\":16, \\\n",
    "# \"PRP\":17, \"PRP$\":18, \"RB\":19, \"RBR\":20, \"RBS\":21, \"RP\":22, \"SYM\":23, \"TO\":24, \\\n",
    "# \"UH\":25, \"VB\":26, \"VBD\":27, \"VBG\":28, \"VBN\":29, \"VBP\":30, \"VBZ\":31, \"WDT\":32, \\\n",
    "# \"WP\":33, \"WP$\":34, \"WRB\":35}\n",
    "\n",
    "# ner2int_simple = {\"\":0,\"FACILITY\":1, \"GPE\":2, \"GSP\":3, \"LOCATION\":4, \"ORGANIZATION\":5, \"PERSON\":6}\n",
    "\n",
    "# s = \"Nancy Smith went to the United States last year and attended McDonalds at 3 o'clock\"\n",
    "# #s = \"bskyb launched its hdtv service , sky+ hd , on 22 may 2006 . prior to its launch , bskyb claimed that 40,000 people had registered to receive the hd service \"\n",
    "\n",
    "# s = s.split()\n",
    "\n",
    "# print(ne_chunk(pos_tag(s)))\n",
    "# print\n",
    "\n",
    "# pos = [pos2int[x[1]] for x in pos_tag(s)]\n",
    "# print(pos)\n",
    "\n",
    "# ner = [ner2int_simple[str(x[2])[2:]] for x in tree2conlltags(ne_chunk(pos_tag(s)))]\n",
    "# print(ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def split_by_whitespace(sentence):\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(re.split(\" \", space_separated_fragment))\n",
    "    return [w for w in words if w]\n",
    "\n",
    "def sentence_to_token_ids(sentence):\n",
    "    \"\"\"Turns an already-tokenized sentence string into word indices\n",
    "    e.g. \"i do n't know\" -> [9, 32, 16, 96]\n",
    "    Note any token that isn't in the word2id mapping gets mapped to the id for UNK\n",
    "    \"\"\"\n",
    "    tokens = split_by_whitespace(sentence) # list of strings\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context_line = \"the service started on 1 september 1993 based on the idea from the then chief executive officer , sam chisholm and rupert murdoch , of converting the company business strategy to an entirely fee-based concept . the new package included four channels formerly available free-to-air , broadcasting on astra 's satellites , as well as introducing new channels . the service continued until the closure of bskyb 's analogue service on 27 september 2001 , due to the launch and expansion of the sky digital platform . some of the channels did broadcast either in the clear or soft encrypted ( whereby a videocrypt decoder was required to decode , without a subscription card ) prior to their addition to the sky multichannels package . within two months of the launch , bskyb gained 400,000 new subscribers , with the majority taking at least one premium channel as well , which helped bskyb reach 3.5 million households by mid-1994 . michael grade criticized the operations in front of the select committee on national heritage , mainly for the lack of original programming on many of the new channels .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nlp = StanfordCoreNLP('http://localhost:9000')\n",
    "# context_tokens = sentence_to_token_ids(context_line)\n",
    "\n",
    "# pos2int = {\"CC\":0, \"CD\":1, \"DT\":2, \"EX\":3, \"FW\":4, \"IN\":5, \"JJ\":6, \"JJR\":7, \"JJS\":8, \\\n",
    "#     \"LS\":9, \"MD\":10, \"NN\":11, \"NNS\":12, \"NNP\":13, \"NNPS\":14, \"PDT\":15, \"POS\":16, \\\n",
    "#     \"PRP\":17, \"PRP$\":18, \"RB\":19, \"RBR\":20, \"RBS\":21, \"RP\":22, \"SYM\":23, \"TO\":24, \\\n",
    "#     \"UH\":25, \"VB\":26, \"VBD\":27, \"VBG\":28, \"VBN\":29, \"VBP\":30, \"VBZ\":31, \"WDT\":32, \\\n",
    "#     \"WP\":33, \"WP$\":34, \"WRB\":35}\n",
    "\n",
    "# ner2int = {\"O\":0, \"PERSON\":1, \"LOCATION\":2, \"ORGANIZATION\":3, \"MISC\":4, \"MONEY\":5, \\\n",
    "#            \"NUMBER\":6, \"ORDINAL\":7, \"PERCENT\":8, \"DATE\":9, \"TIME\":10, \"DURATION\":11, \"SET\":12, \\\n",
    "#            \"EMAIL\":13, \"URL\":14, \"CITY\":15, \"STATE_OR_PROVINCE\":16, \"COUNTRY\":17, \"NATIONALITY\":18, \\\n",
    "#            \"RELIGION\":19, \"TITLE\":20, \"IDEOLOGY\":21, \"CRIMINAL_CHARGE\":22, \"CAUSE_OF_DEATH\":23}\n",
    "\n",
    "# output = nlp.annotate(context_line, properties={\n",
    "#     'annotators': 'pos,ner',\n",
    "#     'tokenize.language': 'Whitespace',\n",
    "#     'outputFormat': 'json'})\n",
    "# ner_tags = np.array([ner2int[str(tok['ner'])] if str(tok['ner']) in ner2int.keys() else 0 for s in output['sentences'] for tok in s['tokens']])\n",
    "# pos_tags = np.array([pos2int[str(tok['pos'])] if str(tok['pos']) in pos2int.keys() else -1 for s in output['sentences'] for tok in s['tokens']])\n",
    "# lems     = [tok['lemma'] for s in output['sentences'] for tok in s['tokens']]\n",
    "# assert (len(ner_tags)==len(context_tokens)), \"%d, %d, %s\" % (len(ner_tags), len(context_tokens), context_line)\n",
    "# assert (len(pos_tags)==len(context_tokens)), \"%d, %d\" % (len(pos_tags), len(context_tokens))\n",
    "# assert (len(lems)==len(context_tokens)),     \"%d, %d\" % (len(lems),     len(context_tokens))\n",
    "\n",
    "\n",
    "# print(context_line)\n",
    "# print(ner_tags)\n",
    "# print(pos_tags)\n",
    "# print(len(ner_tags))\n",
    "# print(len(pos_tags))\n",
    "# print(len(sentence_to_token_ids(context_line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# context_words = ['This','is','a','BART']\n",
    "# context_words_lem = ['this','is','a','bart']\n",
    "# query_words  = ['foo','bar','this']\n",
    "\n",
    "# a = np.array([int(any(context_word==q for q in query_words)) for context_word,context_word_lem in zip(context_words,context_words_lem)]) # original form\n",
    "# b = np.array([int(any(context_word.lower()==q for q in query_words)) for context_word,context_word_lem in zip(context_words,context_words_lem)]) # lower case\n",
    "# c = np.array([int(any(context_word_lem==q for q in query_words)) for context_word,context_word_lem in zip(context_words,context_words_lem)],dtype=np.float32)  # lower case\n",
    "\n",
    "# print(a)\n",
    "# print(b)\n",
    "# print(c)\n",
    "\n",
    "# f = np.concatenate([np.expand_dims(x,1) for x in (a,b,c)],axis=1)\n",
    "# f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['DT', 'NN', 'VBD', 'IN', 'CD', 'NN', 'CD', 'VBN', 'IN', 'DT', 'NN', 'IN', 'DT', 'RB', 'JJ', 'NN', 'NN', ',', 'JJ', 'NN', 'CC', 'NN', 'NN', ',', 'IN', 'VBG', 'DT', 'NN', 'NN', 'NN', 'TO', 'DT', 'RB', 'JJ', 'NN', '.', 'DT', 'JJ', 'NN', 'VBD', 'CD', 'NNS', 'RB', 'JJ', 'NN', ',', 'VBG', 'IN', 'NN', 'POS', 'NNS', ',', 'RB', 'RB', 'IN', 'VBG', 'JJ', 'NNS', '.', 'DT', 'NN', 'VBD', 'IN', 'DT', 'NN', 'IN', 'NN', 'POS', 'JJ', 'NN', 'IN', 'CD', 'NN', 'CD', ',', 'JJ', 'TO', 'DT', 'NN', 'CC', 'NN', 'IN', 'DT', 'JJ', 'JJ', 'NN', '.', 'DT', 'IN', 'DT', 'NNS', 'VBD', 'NN', 'NN', 'IN', 'DT', 'JJ', 'CC', 'JJ', 'VBN', '(', 'WRB', 'DT', 'NN', 'NN', 'VBD', 'VBN', 'TO', 'VB', ',', 'IN', 'DT', 'NN', 'NN', ')', 'RB', 'TO', 'PRP$', 'NN', 'TO', 'DT', 'NN', 'JJ', 'NN', '.', 'IN', 'CD', 'NNS', 'IN', 'DT', 'NN', ',', 'NN', 'VBD', 'CD', 'JJ', 'NNS', ',', 'IN', 'DT', 'NN', 'VBG', 'IN', 'JJS', 'CD', 'JJ', 'NN', 'RB', 'RB', ',', 'WDT', 'VBD', 'VB', 'VB', 'CD', 'CD', 'NNS', 'IN', 'NN', '.', 'NN', 'VBD', 'VBD', 'DT', 'NNS', 'IN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', ',', 'RB', 'IN', 'DT', 'NN', 'IN', 'JJ', 'VBG', 'IN', 'JJ', 'IN', 'DT', 'JJ', 'NNS', '.']\n",
      "['the', 'service', 'start', 'on', '1', 'september', '1993', 'base', 'on', 'the', 'idea', 'from', 'the', 'then', 'chief', 'executive', 'officer', ',', 'sam', 'chisholm', 'and', 'rupert', 'murdoch', ',', 'of', 'convert', 'the', 'company', 'business', 'strategy', 'to', 'an', 'entirely', 'fee-based', 'concept', '.', 'the', 'new', 'package', 'include', 'four', 'channel', 'formerly', 'available', 'free-to-air', ',', 'broadcast', 'on', 'astra', \"'s\", 'satellite', ',', 'as', 'well', 'a', 'introduce', 'new', 'channel', '.', 'the', 'service', 'continue', 'until', 'the', 'closure', 'of', 'bskyb', \"'s\", 'analogue', 'service', 'on', '27', 'september', '2001', ',', 'due', 'to', 'the', 'launch', 'and', 'expansion', 'of', 'the', 'sky', 'digital', 'platform', '.', 'some', 'of', 'the', 'channel', 'do', 'broadcast', 'either', 'in', 'the', 'clear', 'or', 'soft', 'encrypt', '(', 'whereby', 'a', 'videocrypt', 'decoder', 'be', 'require', 'to', 'decode', ',', 'without', 'a', 'subscription', 'card', ')', 'prior', 'to', 'their', 'addition', 'to', 'the', 'sky', 'multichannels', 'package', '.', 'within', 'two', 'month', 'of', 'the', 'launch', ',', 'bskyb', 'gain', '400,000', 'new', 'subscriber', ',', 'with', 'the', 'majority', 'take', 'at', 'least', 'one', 'premium', 'channel', 'as', 'well', ',', 'which', 'help', 'bskyb', 'reach', '3.5', 'million', 'household', 'by', 'mid-1994', '.', 'michael', 'grade', 'criticize', 'the', 'operation', 'in', 'front', 'of', 'the', 'select', 'committee', 'on', 'national', 'heritage', ',', 'mainly', 'for', 'the', 'lack', 'of', 'original', 'program', 'on', 'many', 'of', 'the', 'new', 'channel', '.']\n",
      "189\n",
      "189\n",
      "189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_tokens = split_by_whitespace(context_line)\n",
    "pos_tree = pos_tag(context_tokens)\n",
    "ne_tree = ne_chunk(pos_tree)\n",
    "tree2conlltags(ne_tree)\n",
    "\n",
    "pos2int = {\"CC\":0, \"CD\":1, \"DT\":2, \"EX\":3, \"FW\":4, \"IN\":5, \"JJ\":6, \"JJR\":7, \"JJS\":8, \\\n",
    "    \"LS\":9, \"MD\":10, \"NN\":11, \"NNS\":12, \"NNP\":13, \"NNPS\":14, \"PDT\":15, \"POS\":16, \\\n",
    "    \"PRP\":17, \"PRP$\":18, \"RB\":19, \"RBR\":20, \"RBS\":21, \"RP\":22, \"SYM\":23, \"TO\":24, \\\n",
    "    \"UH\":25, \"VB\":26, \"VBD\":27, \"VBG\":28, \"VBN\":29, \"VBP\":30, \"VBZ\":31, \"WDT\":32, \\\n",
    "    \"WP\":33, \"WP$\":34, \"WRB\":35}\n",
    "\n",
    "ner2int = {\"O\":0, \"PERSON\":1, \"LOCATION\":2, \"ORGANIZATION\":3, \"GSP\":4, \"GPE\":5, \"FACILITY\":6}\n",
    "\n",
    "pos_keys = pos2int.keys()\n",
    "ner_keys = ner2int.keys() \n",
    "\n",
    "pos_tags = [p[1] for p in pos_tag(context_tokens)]\n",
    "ner_tags = [ne[2][2:] for ne in tree2conlltags(ne_chunk(pos_tree))]\n",
    "\n",
    "pos_ids = [pos2int[pos] if pos in pos_keys else -1 for pos in pos_tags]\n",
    "ner_ids = [ner2int[ne]  if ne  in ner_keys else 0  for ne  in ner_tags]\n",
    "            \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lems = [str(lemmatizer.lemmatize(tok,get_wordnet_pos(pos))) if get_wordnet_pos(pos) else str(lemmatizer.lemmatize(tok)) for tok,pos in zip(context_tokens,pos_tags)]\n",
    "\n",
    "print(ner_ids)\n",
    "print(pos_tags)\n",
    "print(lems)\n",
    "\n",
    "print(len(ner_ids))\n",
    "print(len(pos_ids))\n",
    "print(len(lems))\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from nltk.tag import StanfordNERTagger\n",
    "# import time\n",
    "\n",
    "# st = StanfordNERTagger('/Users/andrewweitz/_Stanford/year6/cs224n/stanford-ner-2018-02-27/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "#     '/Users/andrewweitz/_Stanford/year6/cs224n/stanford-ner-2018-02-27/stanford-ner.jar',\n",
    "#     encoding='utf-8')\n",
    "\n",
    "# tic = time.time()\n",
    "# classified_text = st.tag(context_tokens)\n",
    "# print(time.time()-tic)\n",
    "# print(classified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sner import Ner\n",
    "\n",
    "# tagger = Ner(host='localhost',port=9199)\n",
    "\n",
    "# tic = time.time()\n",
    "# t = tagger.get_entities(context_line)\n",
    "# print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# from nltk.tag.perceptron import PerceptronTagger\n",
    "# tagger = PerceptronTagger()\n",
    "# tagset = None\n",
    "# tic = time.time()\n",
    "# tags = nltk.tag._pos_tag(context_tokens, tagset, tagger)\n",
    "# print(time.time()-tic)\n",
    "# tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000380039215088\n",
      "140\n",
      "130\n"
     ]
    }
   ],
   "source": [
    "x = pickle.load( open( \"../output.p\", \"rb\" ) )\n",
    "\n",
    "from vocab import PAD_ID, UNK_ID\n",
    "def padded2(token_batch, num_feats, batch_pad=0, ):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      token_batch: List (length batch size) of arrays.\n",
    "      batch_pad: Int. Length to pad to. If 0, pad to maximum length sequence in token_batch.\n",
    "    Returns:\n",
    "      List (length batch_size) of padded of lists of ints.\n",
    "        All are same length - batch_pad if batch_pad!=0, otherwise the maximum length in token_batch\n",
    "    \"\"\"\n",
    "    maxlen = max(map(lambda x: len(x), token_batch)) if batch_pad == 0 else batch_pad\n",
    "    return map(lambda token_list: token_list + [num_feats*(PAD_ID,)] * (maxlen - len(token_list)) , token_batch)\n",
    "\n",
    "tic = time.time()\n",
    "z = padded2(x,4,600)\n",
    "print(time.time()-tic)\n",
    "\n",
    "y = np.array(z)\n",
    "y.shape\n",
    "\n",
    "f = x[0]\n",
    "print(len(f))\n",
    "f = f[:130]\n",
    "print(len(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00139307975769\n"
     ]
    }
   ],
   "source": [
    "a = 0.4\n",
    "tic = time.time()\n",
    "fdist = FreqDist(context_tokens)\n",
    "tf = [a + (1-a)*fdist[w]/float(max(fdist.values())) for w in context_tokens]\n",
    "print(time.time()-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1.0), (1, 1.0), (1, 1.0)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.5,\n",
       " 0.43333333333333335,\n",
       " 0.6,\n",
       " 0.43333333333333335,\n",
       " 0.4666666666666667,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.6,\n",
       " 1.0,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 1.0,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.7333333333333334,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.4666666666666667,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.7333333333333334,\n",
       " 0.6666666666666667,\n",
       " 0.43333333333333335,\n",
       " 1.0,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.5666666666666667,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.6,\n",
       " 1.0,\n",
       " 0.5333333333333333,\n",
       " 0.4666666666666667,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.5333333333333333,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.7333333333333334,\n",
       " 0.43333333333333335,\n",
       " 0.6,\n",
       " 0.43333333333333335,\n",
       " 0.4666666666666667,\n",
       " 0.43333333333333335,\n",
       " 0.7333333333333334,\n",
       " 0.5,\n",
       " 0.4666666666666667,\n",
       " 0.5,\n",
       " 0.43333333333333335,\n",
       " 0.5333333333333333,\n",
       " 0.5333333333333333,\n",
       " 0.6,\n",
       " 1.0,\n",
       " 0.5,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 1.0,\n",
       " 0.43333333333333335,\n",
       " 0.6666666666666667,\n",
       " 0.5,\n",
       " 0.4666666666666667,\n",
       " 0.43333333333333335,\n",
       " 0.5,\n",
       " 0.6,\n",
       " 0.43333333333333335,\n",
       " 0.4666666666666667,\n",
       " 0.43333333333333335,\n",
       " 0.7333333333333334,\n",
       " 0.43333333333333335,\n",
       " 0.5666666666666667,\n",
       " 1.0,\n",
       " 0.4666666666666667,\n",
       " 0.4666666666666667,\n",
       " 0.43333333333333335,\n",
       " 0.6666666666666667,\n",
       " 1.0,\n",
       " 0.4666666666666667,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.6,\n",
       " 0.43333333333333335,\n",
       " 0.6666666666666667,\n",
       " 1.0,\n",
       " 0.5333333333333333,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.4666666666666667,\n",
       " 1.0,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.4666666666666667,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.5666666666666667,\n",
       " 0.43333333333333335,\n",
       " 0.7333333333333334,\n",
       " 0.43333333333333335,\n",
       " 0.4666666666666667,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.5666666666666667,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.5666666666666667,\n",
       " 1.0,\n",
       " 0.4666666666666667,\n",
       " 0.43333333333333335,\n",
       " 0.4666666666666667,\n",
       " 0.6,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.6666666666666667,\n",
       " 1.0,\n",
       " 0.4666666666666667,\n",
       " 0.7333333333333334,\n",
       " 0.5,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.5333333333333333,\n",
       " 0.43333333333333335,\n",
       " 0.7333333333333334,\n",
       " 0.43333333333333335,\n",
       " 1.0,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.5,\n",
       " 0.4666666666666667,\n",
       " 0.7333333333333334,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.5,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.6,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 1.0,\n",
       " 0.43333333333333335,\n",
       " 0.4666666666666667,\n",
       " 0.43333333333333335,\n",
       " 0.6666666666666667,\n",
       " 1.0,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.6,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.7333333333333334,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 1.0,\n",
       " 0.43333333333333335,\n",
       " 0.6666666666666667,\n",
       " 0.43333333333333335,\n",
       " 0.43333333333333335,\n",
       " 0.6,\n",
       " 0.43333333333333335,\n",
       " 0.6666666666666667,\n",
       " 1.0,\n",
       " 0.5333333333333333,\n",
       " 0.5333333333333333,\n",
       " 0.6]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
